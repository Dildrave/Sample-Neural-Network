{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09460024 -0.17524486 -0.16254544 -0.14943599]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,3) (1,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0acb84daaddd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-0acb84daaddd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0acb84daaddd>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(self, output_layer)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdEdW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0acb84daaddd>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(self, output_layer)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[1;31m# loop through all NON Input layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcurrent_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprevious_layer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m             \u001b[0mcurrent_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_dEdW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdEdW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0acb84daaddd>\u001b[0m in \u001b[0;36mset_dEdW\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_dAdZ\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_dZdW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdEdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdAdZ\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdZdW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m#calculate derivatives for other layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,3) (1,4) "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "#activation function\n",
    "class Sigmoid:\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return (1 / (1 + numpy.exp(-Z)))\n",
    "\n",
    "    # change in sigmoid w.r.t Z\n",
    "    def dsigmoiddZ(self, Z):\n",
    "        return self.sigmoid(Z) * (1 - self.sigmoid(Z))\n",
    "\n",
    "### PrimaryLayer\n",
    "class PrimaryLayer:\n",
    "    \n",
    "    def __init__(self, node_count):\n",
    "        self.node_count = node_count\n",
    "        self.previous_layer = None\n",
    "        self.next_layer = None\n",
    "        self.Z = None\n",
    "        self.A = None\n",
    "    \n",
    "### Secondary :ayer        \n",
    "class SecondaryLayer(PrimaryLayer):\n",
    "    \n",
    "    def __init__(self, node_count, previous_layer):\n",
    "        super().__init__(node_count)\n",
    "        self.dEdW = None\n",
    "        self.set_previous_layer(previous_layer)\n",
    "        self.set_next_layer_to_self()\n",
    "        self.set_W()\n",
    "        self.set_b()\n",
    "        self.set_Z()\n",
    "        self.set_A()\n",
    "\n",
    "    def set_previous_layer(self, previous_layer):\n",
    "        self.previous_layer = previous_layer\n",
    "        \n",
    "    # This points the next_layer of previous_layer to self\n",
    "    def set_next_layer_to_self(self):\n",
    "        self.previous_layer.next_layer = self\n",
    "    \n",
    "    # Randomly generate weights for this layer\n",
    "    def set_W(self):\n",
    "        self.W = numpy.random.random((self.node_count, self.previous_layer.node_count))\n",
    "        \n",
    "    def set_b(self):\n",
    "        self.b = numpy.random.random()\n",
    "        \n",
    "    def set_W_adjusted(self, learning_rate):\n",
    "        self.W_adjusted = W_adjusted\n",
    "        \n",
    "    def set_Z(self):\n",
    "        \n",
    "        previous_layer = self.previous_layer\n",
    "        \n",
    "        if previous_layer.previous_layer is not None:\n",
    "            self.Z = numpy.dot(previous_layer.A, self.W.T) + self.b\n",
    "        else:\n",
    "            #if previous_layer.previos_layer = None then it is the Input Layer\n",
    "            self.Z = numpy.dot(previous_layer.X, self.W.T) + self.b\n",
    "          \n",
    "    def set_A(self):\n",
    "        self.A = Sigmoid().sigmoid(self.Z)\n",
    "        \n",
    "    def set_dAdZ(self):\n",
    "        self.dAdZ = self.A * (1 - self.A)\n",
    "        \n",
    "    def set_dZdW(self):\n",
    "        self.dZdW = self.A\n",
    "        \n",
    "### Input layer\n",
    "class InputLayer(PrimaryLayer):\n",
    "    \n",
    "    def __init__(self, node_count, X):\n",
    "        super().__init__(node_count)\n",
    "        self.X = X\n",
    "    \n",
    "### Output layer\n",
    "class OutputLayer(SecondaryLayer):\n",
    "    \n",
    "    def __init__(self, node_count, previous_layer, Y):   \n",
    "        super().__init__(node_count, previous_layer)\n",
    "        self.Y = Y\n",
    "        self.set_E()\n",
    "        \n",
    "    def set_E(self):\n",
    "        self.E = ((1 / 2) * numpy.square(self.Y - self.A))\n",
    "    \n",
    "    def set_dEdA(self):\n",
    "        self.dEdA = self.A - self.Y\n",
    "    \n",
    "    def set_dEdW(self):\n",
    "        self.set_dEdA()\n",
    "        self.set_dAdZ()\n",
    "        self.set_dZdW()\n",
    "        self.dEdW = self.dEdA * self.dAdZ * self.dZdW\n",
    "        \n",
    "# Hidden Layer\n",
    "class HiddenLayer(SecondaryLayer):\n",
    "    \n",
    "    def __init__(self, node_count, previous_layer):\n",
    "        super().__init__(node_count, previous_layer)\n",
    "    \n",
    "    def set_dEdW(self):\n",
    "        self.set_dAdZ()\n",
    "        self.set_dZdW()\n",
    "        self.dEdW = self.dAdZ * self.dZdW * self.calculate_derivative(self.next_layer)\n",
    "        \n",
    "    #calculate derivatives for other layer    \n",
    "    def calculate_derivative(self, next_layer):\n",
    "        \n",
    "        this_layer = next_layer\n",
    "        \n",
    "        if this_layer.next_layer is None:\n",
    "            \n",
    "        # the outputlayer\n",
    "            this_layer.set_dAdZ()\n",
    "            this_layer.set_dEdA()\n",
    "            dAdZ = this_layer.dAdZ\n",
    "            dEdA = this_layer.dEdA\n",
    "            return dAdZ * dEdA\n",
    "        \n",
    "        elif this_layer is not None:\n",
    "            \n",
    "            this_layer.set_dAdZ()\n",
    "            # How a layer's Z changes w.r.t previous layer's A    \n",
    "            dZdA = this_layer.W.T\n",
    "            # How currrent layer's A changes w.r.t current layer's Z    \n",
    "            dAdZ = this_layer.dAdZ\n",
    "            return dZdA * dAdZ * self.calculate_derivative(this_layer.next_layer)           \n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        X = numpy.array([[1, 1]])\n",
    "        Y = numpy.array([[2, 2, 2, 2]])\n",
    "        \n",
    "        # create layers and foward propagate\n",
    "        self.input_layer = InputLayer(2, X)\n",
    "        self.h1 = HiddenLayer(3, self.input_layer)\n",
    "        self.output_layer = OutputLayer(4, self.h1, Y)\n",
    "         \n",
    "        self.forward_propagation(self.input_layer)\n",
    "        self.back_propagation(self.output_layer)\n",
    "    \n",
    "    def forward_propagation(self, input_layer):\n",
    "        current_layer = input_layer.next_layer     \n",
    "        if current_layer is not None:\n",
    "            current_layer.set_Z()\n",
    "            current_layer.set_A()\n",
    "            \n",
    "            # if current layer is a outputlayer\n",
    "            if current_layer.next_layer is None:\n",
    "                current_layer.set_E()\n",
    "        \n",
    "            self.forward_propagation(current_layer)\n",
    "    \n",
    "    def back_propagation(self, output_layer):\n",
    "        current_layer = output_layer\n",
    "        \n",
    "        # loop through all NON Input layers\n",
    "        if current_layer.previous_layer is not None:\n",
    "            current_layer.set_dEdW()\n",
    "        \n",
    "        print(current_layer.dEdW)\n",
    "        self.back_propagation(current_layer.previous_layer)\n",
    "               \n",
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
